{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Objetivo\n",
        "\n",
        "Nosso objetivo é procurar por informação relativo a itens essencias\n",
        "\n",
        "> numpy:\n",
        "Usar para manipulação aritmética de forma eficiente\n",
        "\n",
        "> pandas:\n",
        "Manipulação de arquivos csv, tabelas, estruturas de dados\n",
        "\n",
        "> duckdb:\n",
        "Roda os arquivos tudo em memória. Rápido e eficiente.\n",
        "\n",
        "> parquet:\n",
        "Armazenamento rápido e eficiente para grandes bases de dados"
      ],
      "metadata": {
        "id": "mvNnbBTBZ5Jp"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qoeakl9tXPdL",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "!pip install duckdb pandas numpy pyarrow"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install scikit-learn unidecode"
      ],
      "metadata": {
        "collapsed": true,
        "id": "n4gXY2Y3a7Y3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install rapidfuzz"
      ],
      "metadata": {
        "collapsed": true,
        "id": "ktX0Uj6L62R1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import duckdb as db\n",
        "import unicodedata\n",
        "import re\n",
        "from rapidfuzz import fuzz\n",
        "from pathlib import Path"
      ],
      "metadata": {
        "id": "qAUpT4PjYOxO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.upload()"
      ],
      "metadata": {
        "id": "60ns-0lGrNdW",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_excel(\"recursos.xlsx\")"
      ],
      "metadata": {
        "collapsed": true,
        "id": "nAQ5C3motKgR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.to_csv(\"converted_recursos.csv\", index=False)"
      ],
      "metadata": {
        "id": "2BhZICNjtg4R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_csv = {\n",
        "    \"disponibilidade1\": \"disponibilidade-jan-abr-2025.csv\",\n",
        "    \"disponibilidade2\": \"disponibilidade-mai-ago-2025.csv\",\n",
        "    \"recursos\": \"converted_recursos.csv\"\n",
        "}"
      ],
      "metadata": {
        "id": "jwxIKt70wN7M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for nome, csv_path in data_csv.items():\n",
        "  parquet_path = Path(csv_path).with_suffix(\".parquet\")\n",
        "\n",
        "  # fazer leitura do CSV com pandas\n",
        "  df = pd.read_csv(csv_path, encoding='latin1') #mudar o enconding\n",
        "\n",
        "  # Salvar agora como parquet\n",
        "                #caminho do parquet\n",
        "  df.to_parquet(parquet_path, engine=\"pyarrow\", compression=\"zstd\")\n",
        "\n",
        "  # se deu certo vamos ver\n",
        "  print(f\"{csv_path} -> {parquet_path}\")"
      ],
      "metadata": {
        "id": "0_eCWfK2wk1C",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "con = db.connect()\n",
        "\n",
        "for nome, csv_path in data_csv.items():\n",
        "  parquet_path = Path(csv_path).with_suffix(\".parquet\")\n",
        "\n",
        "  con.execute(f\"\"\"\n",
        "        CREATE OR REPLACE TABLE \"{nome}\" AS\n",
        "        SELECT * FROM read_parquet('{parquet_path}')\n",
        "    \"\"\")"
      ],
      "metadata": {
        "id": "T3A1HcxmyUUy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Exemplo: contar registros em cada tabela\n",
        "for name in data_csv.keys():\n",
        "    result = con.execute(f\"SELECT COUNT(*) AS total FROM {name}\").fetchdf()\n",
        "    print(name, result)\n"
      ],
      "metadata": {
        "id": "Ur2fOjg2y0NH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1° Pré-processamento dos dados\n",
        "\n",
        "Carregar os dados e fazer um pré-processamento para receber eles da melhor forma possível"
      ],
      "metadata": {
        "id": "3qEzhGZP398a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "path_a = \"disponibilidade-jan-abr-2025.parquet\"\n",
        "path_b = \"converted_recursos.parquet\"\n",
        "\n",
        "df_a = con.execute(f\"SELECT * FROM '{path_a}'\").df()\n",
        "df_b = con.execute(f\"SELECT * FROM '{path_b}'\").df()\n",
        "\n",
        "\n",
        "## Vamos normalizar o texto\n",
        "def norm_text(s: pd.Series) -> pd.Series:\n",
        "    s = s.fillna(\"\").astype(str).str.strip().str.upper()\n",
        "    s = s.map(lambda x: unicodedata.normalize(\"NFKD\", x))\n",
        "    s = s.str.encode(\"ascii\", \"ignore\").str.decode(\"ascii\")\n",
        "    s = s.str.replace(r\"[^A-Z0-9 ]\", \" \", regex=True).str.replace(r\"\\s+\", \" \", regex=True).str.strip()\n",
        "    return s\n",
        "\n",
        "## Agora, normalizar o id também\n",
        "def norm_id(s: pd.Series) -> pd.Series:\n",
        "    s = s.fillna(\"\").astype(str).str.strip().str.upper()\n",
        "    s = s.str.replace(r\"[^A-Z0-9]\", \"\", regex=True)\n",
        "    return s\n",
        "\n",
        "## normaliza as commas e os pontos\n",
        "def extrai_km(x: str) -> float | None:\n",
        "    if not isinstance(x, str): return None\n",
        "    # pega \"KM 123,4\" ou \"km123.4\" etc.\n",
        "    m = re.search(r'KM\\W*([\\d]+(?:[.,]\\d+)?)', x.upper().replace(',', '.'))\n",
        "    return float(m.group(1)) if m else None\n",
        "\n",
        "# padronizações planilha 1 (cadastro/ativos)\n",
        "df_a[\"CONC_N\"]   = norm_text(df_a.get(\"Nome_Concessionaria\", pd.Series()))\n",
        "df_a[\"CONC_ID\"]  = norm_id(df_a.get(\"Cod_Concessionaria\", pd.Series()))\n",
        "df_a[\"ROD_N\"]    = norm_text(df_a.get(\"Rodovia\", pd.Series()))\n",
        "df_a[\"EQUIP_N\"]  = norm_text(df_a.get(\"Desc_Componente\", pd.Series()))\n",
        "df_a[\"NOME_N\"]   = norm_text(df_a.get(\"Nome\", pd.Series()))\n",
        "df_a[\"MARCA_N\"]  = norm_text(df_a.get(\"Marca\", pd.Series()))\n",
        "df_a[\"MODELO_N\"] = norm_text(df_a.get(\"Modelo\", pd.Series()))\n",
        "df_a[\"SENT_N\"]   = norm_text(df_a.get(\"Sentido\", pd.Series()))\n",
        "df_a[\"IDENT_N\"]  = norm_id(df_a.get(\"Identificacao\", pd.Series()))  # candidato a \"código do equipamento\"\n",
        "df_a[\"REGANTT_N\"]= norm_id(df_a.get(\"Cod_Registro_Artesp\", pd.Series()))\n",
        "df_a[\"EXCED1\"]   = df_a.get(\"Excedente\")  # pode ser 0/1, S/N, etc.\n",
        "df_a[\"KM\"]       = df_a.get(\"Localizacao\", pd.Series()).map(lambda x: extrai_km(x if isinstance(x,str) else \"\"))\n",
        "\n",
        "# planilha 2 (medições/ocorrências)\n",
        "df_b[\"CONC_N\"]   = norm_text(df_b.get(\"CONCESSIONÁRIA\", pd.Series()))\n",
        "df_b[\"LOTE_N\"]   = norm_text(df_b.get(\"LOTE\", pd.Series()))\n",
        "df_b[\"EQUIP_N\"]  = norm_text(df_b.get(\"EQUIPAMENTO\", pd.Series()))\n",
        "df_b[\"CODEQ_N\"]  = norm_id(df_b.get(\"COD_EQUIPAMENTO\", pd.Series()))  # principal candidato a Identificacao/ID\n",
        "df_b[\"EXCED2\"]   = df_b.get(\"EXCEDENTE\")\n",
        "df_b[\"VALOR2\"]   = pd.to_numeric(df_b.get(\"VALOR [%]\", pd.Series()).astype(str).str.replace(\",\", \".\"), errors=\"coerce\")\n",
        "# DATA + HORA em timestamp (se existir)\n",
        "if \"DATA\" in df_b.columns:\n",
        "    df_b[\"DATA_TS\"] = pd.to_datetime(df_b[\"DATA\"].astype(str) + \" \" + df_b.get(\"HORA\",\"00:00\").astype(str), errors=\"coerce\")"
      ],
      "metadata": {
        "id": "a2UNMf1rby7k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2° Dicionário de tipos\n",
        "\n",
        "Vamos tentar alinhar rótulos diferentes para o mesmo equipamento"
      ],
      "metadata": {
        "id": "JC281YKL6OdE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# usa nomes diferentes para a mesma coisa (?)\n",
        "APELIDO_TIPO = {\n",
        "    \"CONTROLADOR SEMAFORICO\":\"CONTROLADOR\",\n",
        "    \"PAINEL DE MENSAGEM VARIAVEL\":\"PMV\",\n",
        "    \"PMV\":\"PMV\",\n",
        "    \"RADAR\":\"RADAR\",\n",
        "    \"LOMBADA ELETRONICA\":\"RADAR LOMBADA\",\n",
        "    \"CAMERA\":\"CFTV\",\n",
        "    \"CFTV\":\"CFTV\",\n",
        "}\n",
        "\n",
        "def map_tipo(s: pd.Series) -> pd.Series:\n",
        "    s = s.fillna(\"\").astype(str).str.upper()\n",
        "    return s.map(lambda x: APELIDO_TIPO.get(x, x))\n",
        "\n",
        "df_a[\"TIPO_CAN\"] = map_tipo(df_a[\"EQUIP_N\"].where(df_a[\"EQUIP_N\"]!=\"\", df_a[\"NOME_N\"]))\n",
        "df_b[\"TIPO_CAN\"] = map_tipo(df_b[\"EQUIP_N\"])"
      ],
      "metadata": {
        "id": "vmZ4J64Ib6_A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3° Estratégias de combinação\n",
        "\n",
        "Agora, a partir das variáveis que mais parecem fazer sentido, tentaremos fazer com elas \"batam\"\n",
        "\n",
        "- **Match exato**: o código do aparelho é igual em ambas as planilhas (ÓTIMO)\n",
        "- **Match composto**: mesclaremos diferentes variáveis para tentar chegar em algum lugar"
      ],
      "metadata": {
        "id": "f6T1CBI2_A0n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# MATCH EXATO\n",
        "\n",
        "cand_codes = []\n",
        "if \"IDENT_N\" in df_a and \"CODEQ_N\" in df_b:\n",
        "    cand_codes.append((\"IDENT_N\",\"CODEQ_N\"))\n",
        "if \"REGANTT_N\" in df_a and \"CODEQ_N\" in df_b:\n",
        "    cand_codes.append((\"REGANTT_N\",\"CODEQ_N\"))\n",
        "if \"ID\" in df_a.columns and \"CODEQ_N\" in df_b:\n",
        "    df_a[\"ID_N\"] = norm_id(df_a[\"ID\"])\n",
        "    cand_codes.append((\"ID_N\",\"CODEQ_N\"))\n",
        "\n",
        "matches_exact = []\n",
        "df_a[\"_m1\"] = False\n",
        "df_b[\"_m1\"] = False\n",
        "\n",
        "for a_col,b_col in cand_codes:\n",
        "    m = df_a.loc[~df_a[\"_m1\"]].merge(\n",
        "        df_b.loc[~df_b[\"_m1\"]],\n",
        "        left_on=[a_col,\"CONC_N\",\"TIPO_CAN\"],  # reforço: mesma empresa e mesmo tipo\n",
        "        right_on=[b_col,\"CONC_N\",\"TIPO_CAN\"],\n",
        "        how=\"inner\",\n",
        "        suffixes=(\"_P1\",\"_P2\")\n",
        "    )\n",
        "    if not m.empty:\n",
        "        matches_exact.append(m)\n",
        "        df_a.loc[m.index.get_level_values(0).unique(), \"_m1\"] = True\n",
        "        df_b.loc[m.index.get_level_values(1).unique(), \"_m1\"] = True\n",
        "\n",
        "df_m1 = pd.concat(matches_exact, ignore_index=True) if matches_exact else pd.DataFrame()"
      ],
      "metadata": {
        "id": "YrYGnkpL_dTS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nIgFH91o-_e_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}